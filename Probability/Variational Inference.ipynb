{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler (KL) divergence\n",
    "\n",
    "Given two separate probability distributions $p(z)$ and $q(z)$ over the same random variable $Z$ (i.e. $p$ and $q$ are PMFs if $Z$ is discrete and PDFs otherwise), the Kullback-Leibler (KL) divergence measures how different these two distributions are:\n",
    "\n",
    "$$ D_{KL}(q||p) \\equiv D_{KL}(q(z)||p(z)) = \\mathbb{E}_{Z \\sim q} \\Big[ \\log \\frac{q(z)}{p(z)} \\Big] $$\n",
    "\n",
    "Properties:\n",
    "* $ D_{KL}(q||p)  \\geq 0 $, i.e. the KL-divergence is always non-negative\n",
    "* $ D_{KL}(q||p) = 0 $ iff $p(z) = q(z)$\n",
    "* KL-divergence is not symmetric: $D_{KL}(q||p) \\neq D_{KL}(p||q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-divergence between two Gaussians\n",
    "\n",
    "Let $p(\\mathbf{x}) = N(\\mathbf{x}|\\mu_p,\\Sigma_p)$ and $q(\\mathbf{x}) = N(\\mathbf{x}|\\mu_q,\\Sigma_q)$, both $k$ dimensional Then:\n",
    "\n",
    "$$ \n",
    "D_{KL}(p|q) = \n",
    "\\frac{1}{2} \\Big[ tr(\\Sigma_q^{-1}\\Sigma_p) + (\\mu_q-\\mu_p)^T\\Sigma_q^{-1}(\\mu_q-\\mu_p)-k + \\ln \\frac{\\det\\Sigma_q}{\\det \\Sigma_p} \\Big]\n",
    "$$\n",
    "\n",
    "If $q(\\mathbf{x}) = N(\\mathbf{x}|\\mathbf{0},\\mathbf{I}_k)$, we get:\n",
    "\n",
    "$$ \n",
    "D_{KL}(p|q) = \n",
    "\\frac{1}{2} \\Big[ tr(\\Sigma_p) + \\mu_p^T\\mu_p-k - \\ln (\\det \\Sigma_p) \\Big]\n",
    "$$\n",
    "\n",
    "In the scalar case, where $p(x) = N(x|\\mu_p,\\sigma_p^2)$ and $q(x) = N(x|\\mu_q,\\sigma_q^2)$, we get:\n",
    "\n",
    "$$ \n",
    "D_{KL}(p|q) = \n",
    "\\ln \\frac{\\sigma_q}{\\sigma_p} + \\frac{\\sigma_p^2+(\\mu_p-\\mu_q)^2}{2 \\sigma_q^2} - \\frac{1}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Lower Bound (ELBO)\n",
    "\n",
    "Let $X$ and $Z$ be random variables, jointly distributed with distribution $p$. Then, for any sample $x \\sim p$ and for any distribution $q$, we have:\n",
    "\n",
    "$$ \\ln p(x) \\geq \\mathbb{E}_{q} \\Big[ \\ln \\frac{p(x,z)}{q(z)} \\Big] $$\n",
    "\n",
    "where $p(x)$ is the marginal distribution $p(X)$ evaluated at $X = x$.\n",
    "\n",
    "The quantity \n",
    "\n",
    "$$ \\mathcal{L}(q) \\equiv \\mathbb{E}_{q} \\Big[ \\ln \\frac{p(x,z)}{q(z)} \\Big] = \\mathbb{E}_{q}[\\ln p(x,z)] - \\mathbb{E}_{q}[\\ln q(z)]$$ \n",
    "\n",
    "is the ELBO, also called the variational lower bound.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln p(x) & = \\ln \\int p(x,z) dz \\\\\n",
    "  & = \\ln \\int p(x,z) \\frac{q(z)}{q(z)} dz \\\\\n",
    "  & = \\ln \\int \\frac{p(x,z)}{q(z)} q(z) dz \\\\\n",
    "  & = \\ln \\mathbb{E}_{q} \\Big[ \\ln \\frac{p(x,z)}{q(z)} \\Big] \\\\\n",
    "  & \\geq \\mathbb{E}_{q} \\Big[ \\ln \\frac{p(x,z)}{q(z)} \\Big] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where the final inequality follows from Jensen’s Inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection between ELBO and KL-Divergence\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(q) & = \\mathbb{E}_{q}[\\ln p(x,z)] - \\mathbb{E}_{q}[\\ln q(z)] \\\\\n",
    "\n",
    "  & = \\mathbb{E}_{q}[\\ln p(x|z)p(z)] - \\mathbb{E}_{q}[\\ln q(z)] \\\\\n",
    "\n",
    "  & = \\mathbb{E}_{q}[\\ln p(x|z) + \\ln p(z)] - \\mathbb{E}_{q}[\\ln q(z)]  \\\\\n",
    "  \n",
    "  & = \\mathbb{E}_{q}[\\ln p(x|z)] + \\mathbb{E}_{q}[\\ln p(z) - \\ln q(z)] \\\\\n",
    "\n",
    "  & = \\mathbb{E}_{q}[\\ln p(x|z)] + \\mathbb{E}_{q}[\\ln \\frac{p(z)}{q(z)}] \\\\\n",
    "  \n",
    "  & = \\mathbb{E}_{q}[\\ln p(x|z)] + D_{KL}(q(z)||p(z)) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection between ELBO and the log-evidence\n",
    "\n",
    "The gap between the log-evidence and the ELBO is equal to the KL-divergence:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(q(z)||p(z|x)) & = \\mathbb{E}_{q} \\Big[ \\ln \\frac{q(z)}{p(z|x)} \\Big] \\\\\n",
    "  & = \\mathbb{E}_{q}[\\ln q(z)] - \\mathbb{E}_{q}[\\ln p(z|x)] \\\\\n",
    "  & = \\mathbb{E}_{q}[\\ln q(z)] - \\mathbb{E}_{q}[\\ln \\frac{p(x,z)}{p(x)}] \\\\\n",
    "  & = \\mathbb{E}_{q}[\\ln q(z)] - \\mathbb{E}_{q}[\\ln p(x,z)] + \\mathbb{E}_{q}[\\ln p(x)]\\\\\n",
    "  & = - \\mathbb{E}_{q} \\Big[\\ln \\frac{p(x,z)}{q(z)} \\Big] + \\ln p(x)\\\\\n",
    "  & = - \\mathcal{L}(q) + \\ln p(x)\\\\\n",
    "  & = \\ln p(x) - \\mathcal{L}(q)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "Variational inference estimates a posterior distribution when computing it explicitly is intractable. Variational inference is used in situations in which we have a model that involves hidden random variables $Z$, observed data $X$, and some posited probabilistic model over the hidden and observed random variables $p(X,Z)$. Our goal is to compute the posterior distribution $P(Z|X)$. Ideally, we would do so using Bayes' Theorem:\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$\n",
    "\n",
    "In practice, it is often difficult to compute $p(z|x)$ via Bayes theorem because the denominator $p(x)$ does not have a closed form. Usually $p(x)$ can be only be expressed as an integral that marginalizes over $z$:\n",
    "\n",
    "$$ p(x) = \\int p(x,z) dz $$\n",
    "\n",
    "In such scenarios, we’re often forced to approximate $p(z|x)$ rather than compute it directly. Variational inference is one such approximation technique. Instead of computing $p(z|x)$ exactly via Bayes theorem, variational inference attempts to find another distribution $q(z)$ that is \"close\" to $p(z|x). Ideally, $q(z)$ is easier to evaluate than $p(z|x)$ and, if $p(z|x)$ and $q(z)$ are similar, than we can use $q(z)$ as a replacement for $p(z|x)$ for any relevant downstream tasks. \n",
    "\n",
    "The goal of variational inference is to choose a family $q$ of tractable distributions parameterized by $\\phi$ (e.g. $q$ is the normal distribution and $\\phi$ = $(\\mu,\\sigma)$) and then to find $\\hat{\\phi}$ such that $q_{\\hat{\\phi}}(z)$ is as close to $p(z|x)$ as possible.\n",
    "\n",
    "It would make sense to proceed with this problem by choosing $\\hat{\\phi}$ to minimize the KL-divergence between $q_{\\hat{\\phi}}(z)$ and $p(z|x)$:\n",
    "\n",
    "$$ \\hat{\\phi} = \\underset{\\phi}{argmin} \\ D_{KL}(q_{\\phi}(z)||p(z|x)) $$\n",
    "\n",
    "However, this problem involves $p(z|x)$, which is intractable.\n",
    "\n",
    "Now recall:\n",
    "\n",
    "$$ D_{KL}(q_{\\phi}(z)||p(z|x)) = \\ln p(x) - \\mathcal{L}(q_{\\phi}) $$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\mathcal{L}(q_{\\phi}) = \\mathbb{E}_{q_{\\phi}} \\Big[ \\ln \\frac{p(x,z)}{q_{\\phi}(z)} \\Big] $$\n",
    "\n",
    "is the ELBO.\n",
    "\n",
    "Notice that when it comes to the problem of optimizing $\\phi$, the term $\\ln p(x)$ is a constant. Therefore:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\phi} & = \\underset{\\phi}{argmin} \\ D_{KL}(q_{\\phi}(z)||p(z|x)) \\\\\n",
    "  & = \\underset{\\phi}{argmin} [\\ln p(x) - \\mathcal{L}(q_{\\phi})] \\\\\n",
    "  & = \\underset{\\phi}{argmin} [- \\mathcal{L}(q_{\\phi})] \\\\\n",
    "  & = \\underset{\\phi}{argmax} \\ \\mathcal{L}(q_{\\phi}) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "1. https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
